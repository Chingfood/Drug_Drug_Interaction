{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BioBert_embedding.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM0mgbP16c5R3XMiSlAOCi5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"6368e9f7f5464e139c7344d6b60cf409":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d965f6806a104e899bd7a3237532ea8a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_de0b9dbdbdc643bfa0c4ece2bd709bd8","IPY_MODEL_760245b153724cb78a9ee5041e0ca5b4"]}},"d965f6806a104e899bd7a3237532ea8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"de0b9dbdbdc643bfa0c4ece2bd709bd8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_26e25f27ac17479ba6018392b9229e73","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":462,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":462,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_494f90d8655d4b56a2f78c2bbf0736e4"}},"760245b153724cb78a9ee5041e0ca5b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8ec53f3e85f24c32b6d759b65f0b2a37","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 462/462 [00:00&lt;00:00, 2.79kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_82309466140147a9aa48cd1dceb855ef"}},"26e25f27ac17479ba6018392b9229e73":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"494f90d8655d4b56a2f78c2bbf0736e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8ec53f3e85f24c32b6d759b65f0b2a37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"82309466140147a9aa48cd1dceb855ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"76e55c0472584d91beb5c6e7214d18c8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2e41992cccc741a88f0dbbb04643911f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ec55ed95ed394015bdab8368a758e9a1","IPY_MODEL_a556a16429f84d3c93d07da99ffbdb66"]}},"2e41992cccc741a88f0dbbb04643911f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ec55ed95ed394015bdab8368a758e9a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_da0e47f974c44ba38645054b71d0fed9","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_40c29b043f824eaebb47f9de8c289331"}},"a556a16429f84d3c93d07da99ffbdb66":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a58e6048a8ef4f12874b8592309e4723","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 213k/213k [00:00&lt;00:00, 374kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2303ead1b45e46fc98f982de176e5e1e"}},"da0e47f974c44ba38645054b71d0fed9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"40c29b043f824eaebb47f9de8c289331":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a58e6048a8ef4f12874b8592309e4723":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2303ead1b45e46fc98f982de176e5e1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4723549f2dd94a5fb62199419e506f0e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4cb314b6c82540e2a7c7198458344a3d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b2bd5742b1d642a8bb33aa44590db255","IPY_MODEL_ade18f9e21d345f6b9231dd3797a536c"]}},"4cb314b6c82540e2a7c7198458344a3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b2bd5742b1d642a8bb33aa44590db255":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_885c316b5b5e4e8fb93d2b3fb39b0be3","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":112,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":112,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_95c52d1d84894d40aae0bb8efe831144"}},"ade18f9e21d345f6b9231dd3797a536c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7dc08bae9f7d4521be5fefc68896a273","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 112/112 [00:00&lt;00:00, 521B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6cd49aa26c8d4c2d975d4c96841a5b16"}},"885c316b5b5e4e8fb93d2b3fb39b0be3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"95c52d1d84894d40aae0bb8efe831144":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7dc08bae9f7d4521be5fefc68896a273":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6cd49aa26c8d4c2d975d4c96841a5b16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b9883f59368b44a380abbff77d2c1d46":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b07aa21b23b24d85b46210ae21f3a137","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dc4b917383a942249b1a2d7774afdb8f","IPY_MODEL_b554ba2457f740ddb7d89e1db15b1016"]}},"b07aa21b23b24d85b46210ae21f3a137":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dc4b917383a942249b1a2d7774afdb8f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_38d8938a6cdf4eb387621704c52f8369","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":49,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":49,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2f8df6d133a54ba4b81dadcda096f6ec"}},"b554ba2457f740ddb7d89e1db15b1016":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1de0b3edcabd428fb382982293b4587b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 49.0/49.0 [00:00&lt;00:00, 399B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a46b75c0ff5c4eef8ba94da179e94b15"}},"38d8938a6cdf4eb387621704c52f8369":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2f8df6d133a54ba4b81dadcda096f6ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1de0b3edcabd428fb382982293b4587b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a46b75c0ff5c4eef8ba94da179e94b15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"292674d3141d430ba25122b37c660d35":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cfe0674a8ccc41c49d3a322eb93b480e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c17ae7aa1ee04c6daa52a894a6ad9be9","IPY_MODEL_b212b9bae8a249b49008ab33c3fb7fd6"]}},"cfe0674a8ccc41c49d3a322eb93b480e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c17ae7aa1ee04c6daa52a894a6ad9be9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7b7f3402a4d34254ba847840ae78a285","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433286112,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433286112,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_629d3bb8d74f49989319ab4c02c64536"}},"b212b9bae8a249b49008ab33c3fb7fd6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e2dfe262725e4653811753f2b43c7472","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433M/433M [01:09&lt;00:00, 6.23MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2f9c23c43210441fa5d84a5e7494fd92"}},"7b7f3402a4d34254ba847840ae78a285":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"629d3bb8d74f49989319ab4c02c64536":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e2dfe262725e4653811753f2b43c7472":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2f9c23c43210441fa5d84a5e7494fd92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1bir8OzVhlCN","executionInfo":{"status":"ok","timestamp":1616185068557,"user_tz":240,"elapsed":30021,"user":{"displayName":"Chingyuen Liu","photoUrl":"","userId":"05190519383060638517"}},"outputId":"a8ef4a19-b2ad-4ca7-b1a1-61e5876e4367"},"source":["from google.colab import files, drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RV7vTEQXhzrt","executionInfo":{"status":"ok","timestamp":1616185084410,"user_tz":240,"elapsed":11115,"user":{"displayName":"Chingyuen Liu","photoUrl":"","userId":"05190519383060638517"}},"outputId":"9a155be4-4d2b-46a3-c966-0b2add338539"},"source":["!pip install transformers"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n","\u001b[K     |████████████████████████████████| 2.0MB 5.1MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 21.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 24.5MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=9f9d0c1a787f2b75546c08aab7e2ace9cb1e5403a4199b8fb7529ef78b4eef45\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aSBJ-m8zh2OL","executionInfo":{"status":"ok","timestamp":1616185088215,"user_tz":240,"elapsed":11545,"user":{"displayName":"Chingyuen Liu","photoUrl":"","userId":"05190519383060638517"}}},"source":["from transformers import AutoTokenizer, AutoModel\n","import torch\n","import numpy as np"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGPYe2Mwh-1C","executionInfo":{"status":"ok","timestamp":1616185088219,"user_tz":240,"elapsed":7672,"user":{"displayName":"Chingyuen Liu","photoUrl":"","userId":"05190519383060638517"}}},"source":["import pickle\n","import os\n","def save_obj(obj,path,name):\n","    with open(os.path.join(path, name + '.pkl'), 'wb') as f:\n","        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n","\n","def load_obj(path):\n","    with open(path, 'rb') as f:\n","        return pickle.load(f)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nf-wJ5V9aZD8","executionInfo":{"status":"ok","timestamp":1615954813149,"user_tz":240,"elapsed":642,"user":{"displayName":"Chingyuen Liu","photoUrl":"","userId":"05190519383060638517"}},"outputId":"f659024e-49f8-495c-fb99-c5f1b9395729"},"source":["!ls /content/drive/MyDrive/DDI/test_features/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["bert_1\t\t      BioBert_embedding.ipynb\t  scibert_1\n","bert_2\t\t      data_text_features.pkl\t  scibert_2\n","bert_embedding.ipynb  ddi_test_id_w_features.pkl  scibert_embedding.ipynb\n","biobert_1\t      id_list.pkl\t\t  test_id_w_ground_truth.pkl\n","biobert_2\t      name_list.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PODoJ7mciDNf","executionInfo":{"status":"ok","timestamp":1616185090274,"user_tz":240,"elapsed":7227,"user":{"displayName":"Chingyuen Liu","photoUrl":"","userId":"05190519383060638517"}}},"source":["data_text_features = load_obj(\"/content/drive/MyDrive/DDI/test_features/data_text_features.pkl\")\n","id_list = load_obj(\"/content/drive/MyDrive/DDI/test_features/id_list.pkl\")\n","id_list = [int(_) for _ in id_list]\n","test_id = load_obj(\"/content/drive/MyDrive/DDI/test_features/test_id_w_ground_truth.pkl\")"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBC9M7J6iHA1","executionInfo":{"status":"ok","timestamp":1616185090275,"user_tz":240,"elapsed":5362,"user":{"displayName":"Chingyuen Liu","photoUrl":"","userId":"05190519383060638517"}}},"source":["#Mean Pooling - Take attention mask into account for correct averaging\n","def mean_pooling(model_output, attention_mask):\n","    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n","    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","    return sum_embeddings / sum_mask"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5FsKTt3iIm7","executionInfo":{"status":"ok","timestamp":1616185090276,"user_tz":240,"elapsed":3311,"user":{"displayName":"Chingyuen Liu","photoUrl":"","userId":"05190519383060638517"}}},"source":["def mean_pooling_2(model_output, attention_mask):\n","    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n","    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, [0,1])\n","    sum_mask = torch.clamp(input_mask_expanded.sum([0,1]), min=1e-9)\n","    return sum_embeddings.unsqueeze(0) / sum_mask.unsqueeze(0)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"BONRg0U-iNdz"},"source":["#Sentences we want sentence embeddings for\n","sentences = ['This framework generates embeddings for each input sentence',\n","             'Sentences are passed as a list of string.',\n","             'The quick brown fox jumps over the lazy dog when it is raining.']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q_Pwnv_5hZib","colab":{"base_uri":"https://localhost:8080/","height":260,"referenced_widgets":["6368e9f7f5464e139c7344d6b60cf409","d965f6806a104e899bd7a3237532ea8a","de0b9dbdbdc643bfa0c4ece2bd709bd8","760245b153724cb78a9ee5041e0ca5b4","26e25f27ac17479ba6018392b9229e73","494f90d8655d4b56a2f78c2bbf0736e4","8ec53f3e85f24c32b6d759b65f0b2a37","82309466140147a9aa48cd1dceb855ef","76e55c0472584d91beb5c6e7214d18c8","2e41992cccc741a88f0dbbb04643911f","ec55ed95ed394015bdab8368a758e9a1","a556a16429f84d3c93d07da99ffbdb66","da0e47f974c44ba38645054b71d0fed9","40c29b043f824eaebb47f9de8c289331","a58e6048a8ef4f12874b8592309e4723","2303ead1b45e46fc98f982de176e5e1e","4723549f2dd94a5fb62199419e506f0e","4cb314b6c82540e2a7c7198458344a3d","b2bd5742b1d642a8bb33aa44590db255","ade18f9e21d345f6b9231dd3797a536c","885c316b5b5e4e8fb93d2b3fb39b0be3","95c52d1d84894d40aae0bb8efe831144","7dc08bae9f7d4521be5fefc68896a273","6cd49aa26c8d4c2d975d4c96841a5b16","b9883f59368b44a380abbff77d2c1d46","b07aa21b23b24d85b46210ae21f3a137","dc4b917383a942249b1a2d7774afdb8f","b554ba2457f740ddb7d89e1db15b1016","38d8938a6cdf4eb387621704c52f8369","2f8df6d133a54ba4b81dadcda096f6ec","1de0b3edcabd428fb382982293b4587b","a46b75c0ff5c4eef8ba94da179e94b15","292674d3141d430ba25122b37c660d35","cfe0674a8ccc41c49d3a322eb93b480e","c17ae7aa1ee04c6daa52a894a6ad9be9","b212b9bae8a249b49008ab33c3fb7fd6","7b7f3402a4d34254ba847840ae78a285","629d3bb8d74f49989319ab4c02c64536","e2dfe262725e4653811753f2b43c7472","2f9c23c43210441fa5d84a5e7494fd92"]},"executionInfo":{"status":"ok","timestamp":1616185110293,"user_tz":240,"elapsed":16612,"user":{"displayName":"Chingyuen Liu","photoUrl":"","userId":"05190519383060638517"}},"outputId":"beef1787-bf58-45ec-ef73-2a0b40457886"},"source":["\n","tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n","\n","model = AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\")"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6368e9f7f5464e139c7344d6b60cf409","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=462.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76e55c0472584d91beb5c6e7214d18c8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4723549f2dd94a5fb62199419e506f0e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9883f59368b44a380abbff77d2c1d46","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=49.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"292674d3141d430ba25122b37c660d35","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433286112.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6JHj9Gg4iWGk"},"source":["#Tokenize sentences\n","encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=256, return_tensors='pt')\n","\n","#Compute token embeddings\n","with torch.no_grad():\n","    model_output = model(**encoded_input)\n","\n","#Perform pooling. In this case, mean pooling\n","sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DBdy9dkorLLc"},"source":["data = data_text_features[id_list.index(test_id[6])]\n","data[6]\n","striped_list = [_.strip() for _ in data[6]]\n","full_sent = ' '.join(striped_list)\n","len(full_sent.split())"]},{"cell_type":"markdown","metadata":{"id":"Q6YvoV_juIT8"},"source":["  sl = 0 \n","  i = 8\n","  bert_embedding=[]\n","  bert_embedding_2 = []\n","  if i == 8:\n","      data_list = []\n","      data_list_2 = []\n","      data = data_text_features[id_list.index(test_id[i])]\n","      print(i)\n","      for j in range(2,len(data)):\n","          if j == 4:\n","              print(j)\n","              striped_list = [_.strip() for _ in data[j]]\n","              if len(striped_list) == 0: continue\n","              full_sent = ' '.join(striped_list)\n","              \n","              if len(full_sent.split()) > 512:\n","                  if len(full_sent.split()) > 550:\n","                      strip_index = 0\n","                      strip_count = 0\n","                      for si in range(len(striped_list)):\n","                          strip_count += len(striped_list[si].split())\n","                          if strip_count < 550:\n","                              strip_index = si \n","                          else:\n","                              break\n","                      encoded_input = tokenizer(striped_list[:strip_index], padding=True, truncation=True, max_length=512, return_tensors='pt')\n","                  else:\n","                      encoded_input = tokenizer(striped_list, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","                  #Compute token embeddings\n","                  with torch.no_grad():\n","                      model_output = model(**encoded_input)\n","                  #Perform pooling. In this case, mean pooling\n","                  sentence_embeddings = mean_pooling_2(model_output, encoded_input['attention_mask'])\n","                  data_list.append(sentence_embeddings)\n","                  data_list_2.append(model_output[1].mean(0,keepdim=True))\n","              else:\n","                  encoded_input = tokenizer(full_sent, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","                  #Compute token embeddings\n","                  with torch.no_grad():\n","                      model_output = model(**encoded_input)\n","                  #Perform pooling. In this case, mean pooling\n","                  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","                  data_list.append(sentence_embeddings)\n","                  data_list_2.append(model_output[1])\n","              continue\n","          elif j ==5:\n","              print(j)\n","              striped_list = [_.strip() for _ in data[j]]\n","              if len(striped_list) == 0: continue\n","              full_sent = ' '.join(striped_list)\n","              if len(full_sent.split()) > 512:\n","                  if len(full_sent.split()) > 550:\n","                      strip_index = 0\n","                      strip_count = 0\n","                      for si in range(len(striped_list)):\n","                          strip_count += len(striped_list[si].split())\n","                          if strip_count < 550:\n","                              strip_index = si \n","                          else:\n","                              break\n","                      encoded_input = tokenizer(striped_list[:strip_index], padding=True, truncation=True, max_length=512, return_tensors='pt')\n","                  else:\n","                      encoded_input = tokenizer(striped_list, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","                  #Compute token embeddings\n","                  with torch.no_grad():\n","                      model_output = model(**encoded_input)\n","                  #Perform pooling. In this case, mean pooling\n","                  sentence_embeddings = mean_pooling_2(model_output, encoded_input['attention_mask'])\n","                  data_list.append(sentence_embeddings)\n","                  data_list_2.append(model_output[1].mean(0,keepdim=True))\n","              else:\n","                  encoded_input = tokenizer(full_sent, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","                  #Compute token embeddings\n","                  with torch.no_grad():\n","                      model_output = model(**encoded_input)\n","                  #Perform pooling. In this case, mean pooling\n","                  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","                  data_list.append(sentence_embeddings)\n","                  data_list_2.append(model_output[1])\n","              continue\n","          elif j == 6:\n","              print(j)\n","              striped_list = [_.strip() for _ in data[j]]\n","              if len(striped_list) == 0: continue\n","              full_sent = ' '.join(striped_list)\n","              if len(full_sent.split()) > 800:\n","                  strip_index = 0\n","                  strip_count = 0\n","                  for si in range(len(striped_list)):\n","                      strip_count += len(striped_list[si].split())\n","                      if strip_count < 800:\n","                          strip_index = si \n","                      else:\n","                          break\n","                  encoded_input = tokenizer(striped_list[:strip_index], padding=True, truncation=True, max_length=512, return_tensors='pt')\n","              else:\n","                  encoded_input = tokenizer(striped_list, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","              #Compute token embeddings\n","              with torch.no_grad():\n","                  model_output = model(**encoded_input)\n","              #Perform pooling. In this case, mean pooling\n","              sentence_embeddings = mean_pooling_2(model_output, encoded_input['attention_mask'])\n","              data_list.append(sentence_embeddings)\n","              data_list_2.append(model_output[1].mean(0,keepdim=True))\n","              continue\n","          else:\n","              print(j)\n","              striped_list = [_.strip() for _ in data[j]]\n","              if len(striped_list) == 0: continue\n","              full_sent = ' '.join(striped_list)\n","              encoded_input = tokenizer(full_sent, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","              #Compute token embeddings\n","              with torch.no_grad():\n","                  model_output = model(**encoded_input)\n","              #Perform pooling. In this case, mean pooling\n","              sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","              data_list.append(sentence_embeddings)\n","              data_list_2.append(model_output[1])\n","              continue\n","      bert_embedding.append(np.array(torch.cat(data_list,0)))\n","      bert_embedding_2.append(np.array(torch.cat(data_list_2,0)))\n","  bert_np = np.array(bert_embedding)\n","  bert_np_2 = np.array(bert_embedding_2)\n","  savedex = sl//100\n","  np.save(\"/content/drive/MyDrive/DDI/test_features/biobert_1/\"+str(savedex),bert_np)\n","  np.save(\"/content/drive/MyDrive/DDI/test_features/biobert_2/\"+str(savedex),bert_np_2)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8X3LmX9mIWRY","executionInfo":{"status":"ok","timestamp":1616185110295,"user_tz":240,"elapsed":1838,"user":{"displayName":"Chingyuen Liu","photoUrl":"","userId":"05190519383060638517"}},"outputId":"2cba66df-928d-458d-ac12-4ec5e420d3e2"},"source":["for sl in range(0,len(test_id),100):\n","  print(sl)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["0\n","100\n","200\n","300\n","400\n","500\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HlKiiO-dv_ox","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616185528320,"user_tz":240,"elapsed":363306,"user":{"displayName":"Chingyuen Liu","photoUrl":"","userId":"05190519383060638517"}},"outputId":"02117fb7-ba95-4490-bfab-9b0c5916cee9"},"source":["for sl in range(0,len(test_id),100):\n","  if sl <= 400: continue\n","  bert_embedding=[]\n","  bert_embedding_2 = []\n","  for i in range(sl,min(sl+100,len(test_id))):\n","      data_list = []\n","      data_list_2 = []\n","      data = data_text_features[id_list.index(test_id[i])]\n","      print(i)\n","      \n","      for j in range(2,len(data)):\n","          if j == 4:\n","\n","              striped_list = [_.strip() for _ in data[j]]\n","              if len(striped_list) == 0: continue\n","              full_sent = ' '.join(striped_list)\n","              \n","              if len(full_sent.split()) > 512:\n","                  if len(full_sent.split()) > 1000:\n","                      strip_index = 0\n","                      strip_count = 0\n","                      for si in range(len(striped_list)):\n","                          strip_count += len(striped_list[si].split())\n","                          if strip_count < 1000:\n","                              strip_index = si \n","                          else:\n","                              break\n","                      encoded_input = tokenizer(striped_list[:strip_index], padding=True, truncation=True, max_length=512, return_tensors='pt')\n","                  else:\n","                      encoded_input = tokenizer(striped_list, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","                  \n","                  #Compute token embeddings\n","                  with torch.no_grad():\n","                      model_output = model(**encoded_input)\n","                  #Perform pooling. In this case, mean pooling\n","                  sentence_embeddings = mean_pooling_2(model_output, encoded_input['attention_mask'])\n","                  data_list.append(sentence_embeddings)\n","                  data_list_2.append(model_output[1].mean(0,keepdim=True))\n","              else:\n","                  encoded_input = tokenizer(full_sent, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","                  #Compute token embeddings\n","                  with torch.no_grad():\n","                      model_output = model(**encoded_input)\n","                  #Perform pooling. In this case, mean pooling\n","                  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","                  data_list.append(sentence_embeddings)\n","                  data_list_2.append(model_output[1])\n","              continue\n","          elif j ==5:\n","              striped_list = [_.strip() for _ in data[j]]\n","              if len(striped_list) == 0: continue\n","              full_sent = ' '.join(striped_list)\n","\n","              if len(full_sent.split()) > 512:\n","                  if len(full_sent.split()) > 1000:\n","                      strip_index = 0\n","                      strip_count = 0\n","                      for si in range(len(striped_list)):\n","                          strip_count += len(striped_list[si].split())\n","                          if strip_count < 1000:\n","                              strip_index = si \n","                          else:\n","                              break\n","                      encoded_input = tokenizer(striped_list[:strip_index], padding=True, truncation=True, max_length=512, return_tensors='pt')\n","                  else:\n","                      encoded_input = tokenizer(striped_list, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","                  #Compute token embeddings\n","                  with torch.no_grad():\n","                      model_output = model(**encoded_input)\n","                  #Perform pooling. In this case, mean pooling\n","                  sentence_embeddings = mean_pooling_2(model_output, encoded_input['attention_mask'])\n","                  data_list.append(sentence_embeddings)\n","                  data_list_2.append(model_output[1].mean(0,keepdim=True))\n","              else:\n","                  encoded_input = tokenizer(full_sent, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","                  #Compute token embeddings\n","                  with torch.no_grad():\n","                      model_output = model(**encoded_input)\n","                  #Perform pooling. In this case, mean pooling\n","                  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","                  data_list.append(sentence_embeddings)\n","                  data_list_2.append(model_output[1])\n","              continue\n","          elif j == 6:\n","              striped_list = [_.strip() for _ in data[j]]\n","              if len(striped_list) == 0: continue\n","              full_sent = ' '.join(striped_list)\n","              if len(full_sent.split()) > 1000:\n","                  strip_index = 0\n","                  strip_count = 0\n","                  for si in range(len(striped_list)):\n","                      strip_count += len(striped_list[si].split())\n","                      if strip_count < 1000:\n","                          strip_index = si \n","                      else:\n","                          break\n","                  encoded_input = tokenizer(striped_list[:strip_index], padding=True, truncation=True, max_length=512, return_tensors='pt')\n","              else:\n","                  encoded_input = tokenizer(striped_list, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","              #Compute token embeddings\n","              with torch.no_grad():\n","                  model_output = model(**encoded_input)\n","              #Perform pooling. In this case, mean pooling\n","              sentence_embeddings = mean_pooling_2(model_output, encoded_input['attention_mask'])\n","              data_list.append(sentence_embeddings)\n","              data_list_2.append(model_output[1].mean(0,keepdim=True))\n","              continue\n","          else:\n","              striped_list = [_.strip() for _ in data[j]]\n","              if len(striped_list) == 0: continue\n","              full_sent = ' '.join(striped_list)\n","              encoded_input = tokenizer(full_sent, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","              #Compute token embeddings\n","              with torch.no_grad():\n","                  model_output = model(**encoded_input)\n","              #Perform pooling. In this case, mean pooling\n","              sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","              data_list.append(sentence_embeddings)\n","              data_list_2.append(model_output[1])\n","              continue\n","      bert_embedding.append(np.array(torch.cat(data_list,0)))\n","      bert_embedding_2.append(np.array(torch.cat(data_list_2,0)))\n","  bert_np = np.array(bert_embedding)\n","  bert_np_2 = np.array(bert_embedding_2)\n","  savedex = sl//100\n","  np.save(\"/content/drive/MyDrive/DDI/test_features/biobert_1/GPU/\"+str(savedex),bert_np)\n","  np.save(\"/content/drive/MyDrive/DDI/test_features/biobert_2/GPU/\"+str(savedex),bert_np_2)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["500\n","501\n","502\n","503\n","504\n","505\n","506\n","507\n","508\n","509\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yiy3_j-Yv_my"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"tdy_mjReibSu","executionInfo":{"status":"error","timestamp":1615972592680,"user_tz":240,"elapsed":4149257,"user":{"displayName":"Chingyuen Liu","photoUrl":"","userId":"05190519383060638517"}},"outputId":"2198f2ef-171d-445e-c951-a915e86adcf2"},"source":["for sl in range(0,len(test_id),100):\n","  bert_embedding=[]\n","  bert_embedding_2 = []\n","  for i in range(sl,min(sl+100,len(test_id)):\n","      data_list = []\n","      data_list_2 = []\n","      data = data_text_features[id_list.index(test_id[i])]\n","      print(i)\n","      for j in range(2,len(data)):\n","          if j == 4:\n","\n","              striped_list = [_.strip() for _ in data[j]]\n","              if len(striped_list) == 0: continue\n","              full_sent = ' '.join(striped_list)\n","              \n","              if len(full_sent.split()) > 512:\n","                  encoded_input = tokenizer(full_sent[:512], padding=True, truncation=True, max_length=512, return_tensors='pt')\n","                  #Compute token embeddings\n","                  with torch.no_grad():\n","                      model_output = model(**encoded_input)\n","                  #Perform pooling. In this case, mean pooling\n","                  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","                  data_list.append(sentence_embeddings)\n","                  data_list_2.append(model_output[1].mean(0,keepdim=True))\n","              else:\n","                  encoded_input = tokenizer(full_sent, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","                  #Compute token embeddings\n","                  with torch.no_grad():\n","                      model_output = model(**encoded_input)\n","                  #Perform pooling. In this case, mean pooling\n","                  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","                  data_list.append(sentence_embeddings)\n","                  data_list_2.append(model_output[1])\n","              continue\n","          elif j ==5:\n","              striped_list = [_.strip() for _ in data[j]]\n","              if len(striped_list) == 0: continue\n","              full_sent = ' '.join(striped_list)\n","              if len(full_sent.split()) > 512:\n","                  encoded_input = tokenizer(full_sent[:512], padding=True, truncation=True, max_length=512, return_tensors='pt')\n","                  #Compute token embeddings\n","                  with torch.no_grad():\n","                      model_output = model(**encoded_input)\n","                  #Perform pooling. In this case, mean pooling\n","                  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","                  data_list.append(sentence_embeddings)\n","                  data_list_2.append(model_output[1].mean(0,keepdim=True))\n","              else:\n","                  encoded_input = tokenizer(full_sent, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","                  #Compute token embeddings\n","                  with torch.no_grad():\n","                      model_output = model(**encoded_input)\n","                  #Perform pooling. In this case, mean pooling\n","                  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","                  data_list.append(sentence_embeddings)\n","                  data_list_2.append(model_output[1])\n","              continue\n","          elif j == 6:\n","              striped_list = [_.strip() for _ in data[j]]\n","              if len(striped_list) == 0: continue\n","              full_sent = ' '.join(striped_list)\n","              if len(full_sent.split()) > 512:\n","                  encoded_input = tokenizer(full_sent[:512], padding=True, truncation=True, max_length=512, return_tensors='pt')\n","              else:\n","                  encoded_input = tokenizer(full_sent, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","              #Compute token embeddings\n","              with torch.no_grad():\n","                  model_output = model(**encoded_input)\n","              #Perform pooling. In this case, mean pooling\n","              sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","              data_list.append(sentence_embeddings)\n","              data_list_2.append(model_output[1].mean(0,keepdim=True))\n","              continue\n","          else:\n","              striped_list = [_.strip() for _ in data[j]]\n","              if len(striped_list) == 0: continue\n","              full_sent = ' '.join(striped_list)\n","              encoded_input = tokenizer(full_sent, padding=True, truncation=True, max_length=512, return_tensors='pt')\n","              #Compute token embeddings\n","              with torch.no_grad():\n","                  model_output = model(**encoded_input)\n","              #Perform pooling. In this case, mean pooling\n","              sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","              data_list.append(sentence_embeddings)\n","              data_list_2.append(model_output[1])\n","              continue\n","      bert_embedding.append(np.array(torch.cat(data_list,0)))\n","      bert_embedding_2.append(np.array(torch.cat(data_list_2,0)))\n","  bert_np = np.array(bert_embedding)\n","  bert_np_2 = np.array(bert_embedding_2)\n","  savedex = sl//100\n","  np.save(\"/content/drive/MyDrive/DDI/test_features/biobert_1/\"+str(savedex),bert_np)\n","  np.save(\"/content/drive/MyDrive/DDI/test_features/biobert_2/\"+str(savedex),bert_np_2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n","125\n","126\n","127\n","128\n","129\n","130\n","131\n","132\n","133\n","134\n","135\n","136\n","137\n","138\n","139\n","140\n","141\n","142\n","143\n","144\n","145\n","146\n","147\n","148\n","149\n","150\n","151\n","152\n","153\n","154\n","155\n","156\n","157\n","158\n","159\n","160\n","161\n","162\n","163\n","164\n","165\n","166\n","167\n","168\n","169\n","170\n","171\n","172\n","173\n","174\n","175\n","176\n","177\n","178\n","179\n","180\n","181\n","182\n","183\n","184\n","185\n","186\n","187\n","188\n","189\n","190\n","191\n","192\n","193\n","194\n","195\n","196\n","197\n","198\n","199\n","200\n","201\n","202\n","203\n","204\n","205\n","206\n","207\n","208\n","209\n","210\n","211\n","212\n","213\n","214\n","215\n","216\n","217\n","218\n","219\n","220\n","221\n","222\n","223\n","224\n","225\n","226\n","227\n","228\n","229\n","230\n","231\n","232\n","233\n","234\n","235\n","236\n","237\n","238\n","239\n","240\n","241\n","242\n","243\n","244\n","245\n","246\n","247\n","248\n","249\n","250\n","251\n","252\n","253\n","254\n","255\n","256\n","257\n","258\n","259\n","260\n","261\n","262\n","263\n","264\n","265\n","266\n","267\n","268\n","269\n","270\n","271\n","272\n","273\n","274\n","275\n","276\n","277\n","278\n","279\n","280\n","281\n","282\n","283\n","284\n","285\n","286\n","287\n","288\n","289\n","290\n","291\n","292\n","293\n","294\n","295\n","296\n","297\n","298\n","299\n","300\n","301\n","302\n","303\n","304\n","305\n","306\n","307\n","308\n","309\n","310\n","311\n","312\n","313\n","314\n","315\n","316\n","317\n","318\n","319\n","320\n","321\n","322\n","323\n","324\n","325\n","326\n","327\n","328\n","329\n","330\n","331\n","332\n","333\n","334\n","335\n","336\n","337\n","338\n","339\n","340\n","341\n","342\n","343\n","344\n","345\n","346\n","347\n","348\n","349\n","350\n","351\n","352\n","353\n","354\n","355\n","356\n","357\n","358\n","359\n","360\n","361\n","362\n","363\n","364\n","365\n","366\n","367\n","368\n","369\n","370\n","371\n","372\n","373\n","374\n","375\n","376\n","377\n","378\n","379\n","380\n","381\n","382\n","383\n","384\n","385\n","386\n","387\n","388\n","389\n","390\n","391\n","392\n","393\n","394\n","395\n","396\n","397\n","398\n","399\n","400\n","401\n","402\n","403\n","404\n","405\n","406\n","407\n","408\n","409\n","410\n","411\n","412\n","413\n","414\n","415\n","416\n","417\n","418\n","419\n","420\n","421\n","422\n","423\n","424\n","425\n","426\n","427\n","428\n","429\n","430\n","431\n","432\n","433\n","434\n","435\n","436\n","437\n","438\n","439\n","440\n","441\n","442\n","443\n","444\n","445\n","446\n","447\n","448\n","449\n","450\n","451\n","452\n","453\n","454\n","455\n","456\n","457\n","458\n","459\n","460\n","461\n","462\n","463\n","464\n","465\n","466\n","467\n","468\n","469\n","470\n","471\n","472\n","473\n","474\n","475\n","476\n","477\n","478\n","479\n","480\n","481\n","482\n","483\n","484\n","485\n","486\n","487\n","488\n","489\n","490\n","491\n","492\n","493\n","494\n","495\n","496\n","497\n","498\n","499\n","500\n","501\n","502\n","503\n","504\n","505\n","506\n","507\n","508\n","509\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-9ef7de0fa2f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mdata_list_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_text_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"markdown","metadata":{"id":"jfWfY8lvp4rk"},"source":["6"]},{"cell_type":"code","metadata":{"id":"pn4jV46FeWf9"},"source":["  bert_np = np.array(bert_embedding)\n","  bert_np_2 = np.array(bert_embedding_2)\n","  savedex = sl//100\n","  np.save(\"/content/drive/MyDrive/DDI/test_features/biobert_1/\"+str(savedex),bert_np)\n","  np.save(\"/content/drive/MyDrive/DDI/test_features/biobert_2/\"+str(savedex),bert_np_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RGlO5IKSikQr"},"source":["bert_np = np.array(bert_embedding)\n","bert_np_2 = np.array(bert_embedding_2)\n","np.save(\"/content/drive/MyDrive/DDI/training/BioBert/0\",bert_np[:-1,:,:])\n","np.save(\"/content/drive/MyDrive/DDI/training/BioBert_2/0\",bert_np_2[:-1,:,:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ys3Er_qyyW-Y","executionInfo":{"elapsed":267,"status":"ok","timestamp":1615944507474,"user":{"displayName":"Chingyuen Liu","photoUrl":"","userId":"05190519383060638517"},"user_tz":240},"outputId":"6117d9c9-1cc4-4e45-ed42-2906dfbe9961"},"source":["bert_np[:-1,:,:].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(220, 5, 768)"]},"metadata":{"tags":[]},"execution_count":19}]}]}